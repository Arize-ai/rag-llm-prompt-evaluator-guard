{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAuY_sNgr4OQ"
      },
      "source": [
        "# Install Dependencies\n",
        "Various installations are required for OTL, LlamaIndex and Open AI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HY97cJFAk7Sc"
      },
      "outputs": [],
      "source": [
        "!pip install -qq 'openinference-instrumentation-llama-index>=0.1.6' 'openinference-instrumentation-llama-index>=0.1.6'  llama-index-llms-openai opentelemetry-exporter-otlp llama-index>=0.10.3 \"llama-index-callbacks-arize-phoenix>=0.1.2\" arize-otel\n",
        "\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZvseSFHsD7M"
      },
      "source": [
        "# Initialize Arize Phoenix\n",
        "Set up OTL tracer for the `LlamaIndexInstrumentor`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCDUdapRrqpd"
      },
      "outputs": [],
      "source": [
        "from openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n",
        "from arize_otel import register_otel, Endpoints\n",
        "\n",
        "# Setup OTEL via our convenience function\n",
        "register_otel(\n",
        "    endpoints = Endpoints.ARIZE,\n",
        "    space_key = getpass(\"ðŸ”‘ Enter your Arize space key in the space settings page of the Arize UI: \"),\n",
        "    api_key = getpass(\"ðŸ”‘ Enter your Arize API key in the space settings page of the Arize UI: \"),\n",
        "    model_id = \"test-guard-july10-6:07pm\", # name this to whatever you would like\n",
        ")\n",
        "LlamaIndexInstrumentor().instrument()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0z-3-Reas5TW"
      },
      "source": [
        "# Instrument Guardrails AI\n",
        "Install and instrument Guardrails AI. Import `ArizeDatasetEmbeddings` Guard."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq guardrails-ai litellm"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Q8nwDMRXfmjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --no-cache-dir -qq git+https://github.com/Arize-ai/rag-llm-prompt-evaluator-guard\n",
        "!guardrails hub install hub://arize-ai/llm_rag_evaluator\n",
        "\n",
        "from guardrails.hub import LlmRagEvaluator"
      ],
      "metadata": {
        "id": "TK22jVYzBTJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5GV4hwGUxxB"
      },
      "outputs": [],
      "source": [
        "from guardrails import Guard\n",
        "\n",
        "guard = Guard.from_string(\n",
        "            validators=[\n",
        "                LlmRagEvaluator(\n",
        "                    eval_llm_prompt_generator=HallucinationPrompt(prompt_name=\"hallucination_judge_llm\"),\n",
        "                    llm_evaluator_fail_response=\"hallucinated\",\n",
        "                    llm_evaluator_pass_response=\"factual\",\n",
        "                    llm_callable=\"gpt-4o-mini\",\n",
        "                    on_fail=\"exception\",\n",
        "                    on=\"prompt\")\n",
        "            ],\n",
        "        )\n",
        "guard._disable_tracer = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpwPFAtDsrcf"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "from typing import Optional, List, Mapping, Any\n",
        "\n",
        "from llama_index.core import SimpleDirectoryReader, SummaryIndex\n",
        "from llama_index.core.callbacks import CallbackManager\n",
        "from llama_index.core.llms import (\n",
        "    CustomLLM,\n",
        "    CompletionResponse,\n",
        "    CompletionResponseGen,\n",
        "    LLMMetadata,\n",
        ")\n",
        "from llama_index.core.llms.callbacks import llm_completion_callback\n",
        "from llama_index.core import Settings\n",
        "\n",
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "def monkey_completion(prompt, **kwargs):\n",
        "    _, _, context_component_of_prompt = prompt.partition(\"Context information is below.\")\n",
        "    _, _, query_component_of_prompt = prompt.partition(\"Query: \")\n",
        "    return guard(\n",
        "      llm_api=openai.chat.completions.create,\n",
        "      prompt=prompt,\n",
        "      model=\"gpt-3.5-turbo\",\n",
        "      max_tokens=1024,\n",
        "      temperature=0.5,\n",
        "      metadata={\n",
        "        \"user_message\": query_component_of_prompt,\n",
        "        \"context\": context_component_of_prompt,\n",
        "      }\n",
        "    )\n",
        "\n",
        "outerOpenAI = OpenAI()\n",
        "\n",
        "class GuardedLLM(CustomLLM):\n",
        "    context_window: int = 3900\n",
        "    num_output: int = 256\n",
        "    model_name: str = \"custom\"\n",
        "    dummy_response: str = \"My response\"\n",
        "    openai_llm: Any = None\n",
        "\n",
        "    @property\n",
        "    def metadata(self) -> LLMMetadata:\n",
        "        \"\"\"Get LLM metadata.\"\"\"\n",
        "        return outerOpenAI.metadata\n",
        "\n",
        "    @llm_completion_callback()\n",
        "    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:\n",
        "        validated_response = monkey_completion(prompt, **kwargs)\n",
        "        return CompletionResponse(text=validated_response.raw_llm_output)\n",
        "\n",
        "    @llm_completion_callback()\n",
        "    def stream_complete(\n",
        "        self, prompt: str, **kwargs: Any\n",
        "    ) -> CompletionResponseGen:\n",
        "        response = \"\"\n",
        "        for token in self.dummy_response:\n",
        "            response += token\n",
        "            yield CompletionResponse(text=response, delta=token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2nKIt4duKAU"
      },
      "source": [
        "# Set Up RAG Application\n",
        "Create a LlamaIndex VectorStore to create a classic RAG application over Paul Graham essays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-g6O8tvsxKd"
      },
      "outputs": [],
      "source": [
        "!mkdir -p 'data/paul_graham/'\n",
        "!curl 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' > 'data/paul_graham/paul_graham_essay.txt'\n",
        "import logging\n",
        "import sys\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
        "\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "# load documents\n",
        "documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\n",
        "index = VectorStoreIndex.from_documents(documents, chunk_size=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1Rzk87ZwDo8"
      },
      "outputs": [],
      "source": [
        "from guardrails.errors import ValidationError\n",
        "\n",
        "query_engine = index.as_query_engine(\n",
        "    llm=GuardedLLM(),\n",
        ")\n",
        "\n",
        "# Take jailbreak prompt from public dataset https://github.com/verazuo/jailbreak_llms/blob/main/data/prompts/jailbreak_prompts_2023_05_07.csv\n",
        "sample_prompt = \"\"\"What are the 3 ingredients to great work?\"\"\"\n",
        "\n",
        "response = query_engine.query(sample_prompt)\n",
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}