{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAuY_sNgr4OQ"
      },
      "source": [
        "# Install Dependencies\n",
        "Various installations are required for OTL, LlamaIndex and Open AI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HY97cJFAk7Sc"
      },
      "outputs": [],
      "source": [
        "!pip install -qq 'openinference-instrumentation-llama-index>=0.1.6' 'openinference-instrumentation-llama-index>=0.1.6'  llama-index-llms-openai opentelemetry-exporter-otlp llama-index>=0.10.3 \"llama-index-callbacks-arize-phoenix>=0.1.2\" arize-otel\n",
        "\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZvseSFHsD7M"
      },
      "source": [
        "# Initialize Arize Phoenix\n",
        "Set up OTL tracer for the `LlamaIndexInstrumentor`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCDUdapRrqpd"
      },
      "outputs": [],
      "source": [
        "from openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n",
        "from arize_otel import register_otel, Endpoints\n",
        "\n",
        "# Setup OTEL via our convenience function\n",
        "register_otel(\n",
        "    endpoints = Endpoints.ARIZE,\n",
        "    space_key = getpass(\"ðŸ”‘ Enter your Arize space key in the space settings page of the Arize UI: \"),\n",
        "    api_key = getpass(\"ðŸ”‘ Enter your Arize API key in the space settings page of the Arize UI: \"),\n",
        "    model_id = \"test-guard-july10-6:07pm\", # name this to whatever you would like\n",
        ")\n",
        "LlamaIndexInstrumentor().instrument()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0z-3-Reas5TW"
      },
      "source": [
        "# Instrument Guardrails AI\n",
        "Install and instrument Guardrails AI. Import `ArizeDatasetEmbeddings` Guard."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq guardrails-ai litellm"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Q8nwDMRXfmjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import Any, Callable, Dict, Optional, Type\n",
        "import logging\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "from guardrails.validator_base import (\n",
        "    FailResult,\n",
        "    PassResult,\n",
        "    ValidationResult,\n",
        "    Validator,\n",
        "    register_validator,\n",
        ")\n",
        "from guardrails.stores.context import get_call_kwarg\n",
        "from litellm import completion, get_llm_provider\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class ArizeRagEvalPromptBase(ABC):\n",
        "    def __init__(self, prompt_name, **kwargs) -> None:\n",
        "        self.prompt_name = prompt_name\n",
        "\n",
        "    @abstractmethod\n",
        "    def generate_prompt(self, user_input_message: str, reference_text: str, llm_response: str) -> str:\n",
        "        pass\n",
        "\n",
        "\n",
        "class HallucinationPrompt(ArizeRagEvalPromptBase):\n",
        "    def generate_prompt(self, user_input_message: str, reference_text: str, llm_response: str) -> str:\n",
        "        return f\"\"\"\n",
        "            In this task, you will be presented with a query, a reference text and an answer. The answer is\n",
        "            generated to the question based on the reference text. The answer may contain false information. You\n",
        "            must use the reference text to determine if the answer to the question contains false information,\n",
        "            if the answer is a hallucination of facts. Your objective is to determine whether the answer text\n",
        "            contains factual information and is not a hallucination. A 'hallucination' refers to\n",
        "            an answer that is not based on the reference text or assumes information that is not available in\n",
        "            the reference text. Your response should be a single word: either \"factual\" or \"hallucinated\", and\n",
        "            it should not include any other text or characters. \"hallucinated\" indicates that the answer\n",
        "            provides factually inaccurate information to the query based on the reference text. \"factual\"\n",
        "            indicates that the answer to the question is correct relative to the reference text, and does not\n",
        "            contain made up information. Please read the query and reference text carefully before determining\n",
        "            your response.\n",
        "\n",
        "                [BEGIN DATA]\n",
        "                ************\n",
        "                [Query]: {user_input_message}\n",
        "                ************\n",
        "                [Reference text]: {reference_text}\n",
        "                ************\n",
        "                [Answer]: {llm_response}\n",
        "                ************\n",
        "                [END DATA]\n",
        "\n",
        "                Is the answer above factual or hallucinated based on the query and reference text?\n",
        "            \"\"\"\n",
        "\n",
        "\n",
        "@register_validator(name=\"arize/llm_rag_evaluator\", data_type=\"string\")\n",
        "class LlmRagEvaluator(Validator):\n",
        "    \"\"\"This class validates an output generated by a LiteLLM (LLM) model by prompting another LLM model to evaluate the output.\n",
        "\n",
        "    **Key Properties**\n",
        "\n",
        "    | Property                      | Description                       |\n",
        "    | ----------------------------- | --------------------------------- |\n",
        "    | Name for `format` attribute   | `arize/relevancy_evaluator`       |\n",
        "    | Supported data types          | `string`                          |\n",
        "    | Programmatic fix              | N/A                               |\n",
        "\n",
        "    Args:\n",
        "        llm_callable (str, optional): The name of the LiteLLM model to use for validation. Defaults to \"gpt-3.5-turbo\".\n",
        "        on_fail (Callable, optional): A function to be called when validation fails. Defaults to None.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        eval_llm_prompt_generator: Type[ArizeRagEvalPromptBase],\n",
        "        llm_evaluator_fail_response: str,\n",
        "        llm_evaluator_pass_response: str,\n",
        "        llm_callable: str,\n",
        "        on_fail: Optional[Callable] = \"noop\",\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(\n",
        "            on_fail,\n",
        "            eval_llm_prompt_generator=eval_llm_prompt_generator,\n",
        "            llm_evaluator_fail_response=llm_evaluator_fail_response,\n",
        "            llm_evaluator_pass_response=llm_evaluator_pass_response,\n",
        "            llm_callable=llm_callable,\n",
        "            **kwargs)\n",
        "        self._llm_evaluator_prompt_generator = eval_llm_prompt_generator\n",
        "        self._llm_callable = llm_callable\n",
        "        self._fail_response = llm_evaluator_fail_response\n",
        "        self._pass_response = llm_evaluator_pass_response\n",
        "\n",
        "    def get_llm_response(self, prompt: str) -> str:\n",
        "        \"\"\"Gets the response from the LLM.\n",
        "\n",
        "        Args:\n",
        "            prompt (str): The prompt to send to the LLM.\n",
        "\n",
        "        Returns:\n",
        "            str: The response from the LLM.\n",
        "        \"\"\"\n",
        "        # 0. Create messages\n",
        "        messages = [{\"content\": prompt, \"role\": \"user\"}]\n",
        "\n",
        "        # 0b. Setup auth kwargs if the model is from OpenAI\n",
        "        kwargs = {}\n",
        "        _model, provider, *_rest = get_llm_provider(self._llm_callable)\n",
        "        if provider == \"openai\":\n",
        "            kwargs[\"api_key\"] = get_call_kwarg(\"api_key\") or os.environ.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "        # 1. Get LLM response\n",
        "        # Strip whitespace and convert to lowercase\n",
        "        try:\n",
        "            response = completion(model=self._llm_callable, messages=messages, **kwargs)\n",
        "            response = response.choices[0].message.content  # type: ignore\n",
        "            response = response.strip().lower()\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Error getting response from the LLM: {e}\") from e\n",
        "\n",
        "        # 3. Return the response\n",
        "        return response\n",
        "\n",
        "    def validate(self, value: Any, metadata: Dict) -> ValidationResult:\n",
        "        \"\"\"\n",
        "        Validates is based on the relevance of the reference text to the original question.\n",
        "\n",
        "        Args:\n",
        "            value (Any): The value to validate. It must contain 'original_prompt' and 'reference_text' keys.\n",
        "            metadata (Dict): The metadata for the validation.\n",
        "                user_message: Required key. User query passed into RAG LLM.\n",
        "                context: Required key. Context used by RAG LLM.\n",
        "                llm_response: Optional key. By default, the gaurded LLM will make the RAG LLM call, which corresponds\n",
        "                    to the `value`. If the user calls the guard with on=\"prompt\", then the original RAG LLM response\n",
        "                    needs to be passed into the guard as metadata for the LLM judge to evaluate.\n",
        "\n",
        "        Returns:\n",
        "            ValidationResult: The result of the validation. It can be a PassResult if the reference\n",
        "                              text is relevant to the original question, or a FailResult otherwise.\n",
        "        \"\"\"\n",
        "        # 1. Get the question and arg from the value\n",
        "        user_input_message = metadata.get(\"user_message\")\n",
        "        if user_input_message is None:\n",
        "            raise RuntimeError(\n",
        "                \"original_prompt missing from value. \"\n",
        "                \"Please provide the original prompt.\"\n",
        "            )\n",
        "\n",
        "        reference_text = metadata.get(\"context\")\n",
        "        if reference_text is None:\n",
        "            raise RuntimeError(\n",
        "                \"'reference_text' missing from value. \"\n",
        "                \"Please provide the reference text.\"\n",
        "            )\n",
        "\n",
        "        # Option to override guarded LLM call with response passed in through metadata\n",
        "        if metadata.get(\"llm_response\") is not None:\n",
        "            value = metadata.get(\"llm_response\")\n",
        "\n",
        "        # 2. Setup the prompt\n",
        "        prompt = self._llm_evaluator_prompt_generator.generate_prompt(user_input_message=user_input_message, reference_text=reference_text, llm_response=value)\n",
        "        print(f\"\\nevaluator prompt: {prompt}\")\n",
        "\n",
        "        # 3. Get the LLM response\n",
        "        llm_response = self.get_llm_response(prompt)\n",
        "        print(f\"\\nllm evaluator response: {llm_response}\")\n",
        "\n",
        "        # 4. Check the LLM response and return the result\n",
        "        if llm_response == self._fail_response:\n",
        "            print(f\"\\nVALIDATION FAILED\")\n",
        "            return FailResult(error_message=f\"The LLM says {self._fail_response}. The validation failed.\")\n",
        "\n",
        "        if llm_response == self._pass_response:\n",
        "            print(f\"\\nVALIDATION PASSED\")\n",
        "            return PassResult()\n",
        "\n",
        "        print(f\"\\nVALIDATION FAILED\")\n",
        "        return FailResult(\n",
        "            error_message=\"The LLM returned an invalid answer. Failing the validation...\"\n",
        "        )\n"
      ],
      "metadata": {
        "id": "WIdK2paCf828"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !guardrails hub install hub://arize-ai/llm_rag_evaluator\n",
        "\n",
        "# from guardrails.hub import LlmRagEvaluator"
      ],
      "metadata": {
        "id": "TK22jVYzBTJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5GV4hwGUxxB"
      },
      "outputs": [],
      "source": [
        "from guardrails import Guard\n",
        "\n",
        "guard = Guard.from_string(\n",
        "            validators=[\n",
        "                LlmRagEvaluator(\n",
        "                    eval_llm_prompt_generator=HallucinationPrompt(prompt_name=\"hallucination_judge_llm\"),\n",
        "                    llm_evaluator_fail_response=\"hallucinated\",\n",
        "                    llm_evaluator_pass_response=\"factual\",\n",
        "                    llm_callable=\"gpt-4o-mini\",\n",
        "                    on_fail=\"exception\",\n",
        "                    on=\"prompt\")\n",
        "            ],\n",
        "        )\n",
        "guard._disable_tracer = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpwPFAtDsrcf"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "from typing import Optional, List, Mapping, Any\n",
        "\n",
        "from llama_index.core import SimpleDirectoryReader, SummaryIndex\n",
        "from llama_index.core.callbacks import CallbackManager\n",
        "from llama_index.core.llms import (\n",
        "    CustomLLM,\n",
        "    CompletionResponse,\n",
        "    CompletionResponseGen,\n",
        "    LLMMetadata,\n",
        ")\n",
        "from llama_index.core.llms.callbacks import llm_completion_callback\n",
        "from llama_index.core import Settings\n",
        "\n",
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "def monkey_completion(prompt, **kwargs):\n",
        "    _, _, context_component_of_prompt = prompt.partition(\"Context information is below.\")\n",
        "    _, _, query_component_of_prompt = prompt.partition(\"Query: \")\n",
        "    return guard(\n",
        "      llm_api=openai.chat.completions.create,\n",
        "      prompt=prompt,\n",
        "      model=\"gpt-3.5-turbo\",\n",
        "      max_tokens=1024,\n",
        "      temperature=0.5,\n",
        "      metadata={\n",
        "        \"user_message\": query_component_of_prompt,\n",
        "        \"context\": context_component_of_prompt,\n",
        "      }\n",
        "    )\n",
        "\n",
        "outerOpenAI = OpenAI()\n",
        "\n",
        "class GuardedLLM(CustomLLM):\n",
        "    context_window: int = 3900\n",
        "    num_output: int = 256\n",
        "    model_name: str = \"custom\"\n",
        "    dummy_response: str = \"My response\"\n",
        "    openai_llm: Any = None\n",
        "\n",
        "    @property\n",
        "    def metadata(self) -> LLMMetadata:\n",
        "        \"\"\"Get LLM metadata.\"\"\"\n",
        "        return outerOpenAI.metadata\n",
        "\n",
        "    @llm_completion_callback()\n",
        "    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:\n",
        "        validated_response = monkey_completion(prompt, **kwargs)\n",
        "        return CompletionResponse(text=validated_response.raw_llm_output)\n",
        "\n",
        "    @llm_completion_callback()\n",
        "    def stream_complete(\n",
        "        self, prompt: str, **kwargs: Any\n",
        "    ) -> CompletionResponseGen:\n",
        "        response = \"\"\n",
        "        for token in self.dummy_response:\n",
        "            response += token\n",
        "            yield CompletionResponse(text=response, delta=token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2nKIt4duKAU"
      },
      "source": [
        "# Set Up RAG Application\n",
        "Create a LlamaIndex VectorStore to create a classic RAG application over Paul Graham essays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-g6O8tvsxKd"
      },
      "outputs": [],
      "source": [
        "!mkdir -p 'data/paul_graham/'\n",
        "!curl 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' > 'data/paul_graham/paul_graham_essay.txt'\n",
        "import logging\n",
        "import sys\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
        "\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "# load documents\n",
        "documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\n",
        "index = VectorStoreIndex.from_documents(documents, chunk_size=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1Rzk87ZwDo8"
      },
      "outputs": [],
      "source": [
        "from guardrails.errors import ValidationError\n",
        "\n",
        "query_engine = index.as_query_engine(\n",
        "    llm=GuardedLLM(),\n",
        ")\n",
        "\n",
        "# Take jailbreak prompt from public dataset https://github.com/verazuo/jailbreak_llms/blob/main/data/prompts/jailbreak_prompts_2023_05_07.csv\n",
        "sample_prompt = \"\"\"What are the 3 ingredients to great work?\"\"\"\n",
        "\n",
        "response = query_engine.query(sample_prompt)\n",
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}